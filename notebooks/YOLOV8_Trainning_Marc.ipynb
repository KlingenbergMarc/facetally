{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Input: train and val preprocessed data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# To Do, replace this\n",
        "train_ds = 0\n",
        "val_ds = 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from face_tally.params import *\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import keras_cv\n",
        "from keras_cv import bounding_box, visualization\n",
        "from ultralytics import YOLO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define the number of classes for the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class_ids = ['face']\n",
        "class_mapping = dict(zip(range(len(class_ids)), class_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import a backone for the model. Start with small one from coco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-cv/models/yolov8/coco/yolov8_s_backbone.h5\n",
            "20596968/20596968 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "backbone = keras_cv.models.YOLOV8Backbone.from_preset(\n",
        "    \"yolo_v8_s_backbone_coco\"  # We will use yolov8 small backbone with coco weights\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define YOLO model with face class and COCO backbone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo = keras_cv.models.YOLOV8Detector(\n",
        "    num_classes=len(class_mapping),\n",
        "    bounding_box_format=\"xyxy\",\n",
        "    backbone=backbone,\n",
        "    fpn_depth=1,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compile the model with binary_crossentropy and Complete IoU (CIoU) metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    global_clipnorm=GLOBAL_CLIPNORM,\n",
        ")\n",
        "\n",
        "yolo.compile(\n",
        "    optimizer=optimizer, classification_loss=\"binary_crossentropy\", box_loss=\"ciou\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yolo.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=3\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model():\n",
        "    # Load the pre-trained model\n",
        "    model = YOLO(\"yolov8s-p2.yaml\").load(\"yolov8s.pt\")\n",
        "\n",
        "    # Train the model\n",
        "    model.train(\n",
        "        data=\"dataset.yaml\", epochs=200,  imgsz=256, save=True, format='onnx'\n",
        "    )  # Set imgsz to 256 for training on 256x256 images\n",
        "\n",
        "    # Export the model to ONNX format\n",
        "    path = model.export()\n",
        "    print(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example of predictions\n",
        "model = YOLO('best-30.pt') # Load this from\n",
        "results = model('yolo/images/test/image_4008.png', imgsz=256, save=True, conf=0.5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_detections(model, dataset, bounding_box_format):\n",
        "    images, y_true = next(iter(dataset.take(1)))\n",
        "    y_pred = model.predict(images)\n",
        "    y_pred = bounding_box.to_ragged(y_pred)\n",
        "    visualization.plot_bounding_box_gallery(\n",
        "        images,\n",
        "        value_range=(0, 255),\n",
        "        bounding_box_format=bounding_box_format,\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        scale=4,\n",
        "        rows=2,\n",
        "        cols=2,\n",
        "        show=True,\n",
        "        font_scale=0.7,\n",
        "        class_mapping=class_mapping,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_detections(yolo, dataset=val_ds, bounding_box_format=\"xyxy\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lewagon",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
